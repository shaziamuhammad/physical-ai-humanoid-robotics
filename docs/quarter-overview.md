---
id: quarter-overview
title: "Quarter Overview"
---

## What is Physical AI?

Physical AI refers to artificial intelligence systems that interact with the real world through physical bodies, such as robots. Unlike purely digital AI, Physical AI operates in dynamic, unstructured environments, requiring capabilities like perception, manipulation, and navigation.

## Humanoid Robots and Embodied Intelligence

Humanoid robots are a key platform for Physical AI due to their human-like form and ability to operate in environments designed for humans. Embodied intelligence emphasizes that intelligence is deeply intertwined with an agent's physical body and its interactions with the environment. Humanoids provide a rich testbed for developing AI that understands and acts in the physical world.

## Module Summaries

This quarter's capstone project is structured into four core modules, each building upon the last to provide a comprehensive understanding of Physical AI and humanoid robotics:

### Module 1: ROS 2

Focuses on the Robotic Operating System 2 (ROS 2), the foundational middleware for robotic applications. This module covers core concepts like nodes, topics, services, and actions, and introduces how to build an LLM/agent-to-ROS bridge.
- ROS 2 concepts: nodes, topics, services, actions
- rclpy basics
- URDF for humanoid robots

### Module 2: Digital Twin

Explores the creation and utilization of digital twins for robotics. This involves using Gazebo for high-fidelity physics simulations and Unity for advanced visualization and human-robot interaction, along with integrating various sensors.
- Gazebo physics and simulation
- Sensors: LiDAR, depth camera, IMU
- Unity as a visualization/interaction layer

### Module 3: NVIDIA Isaac

Delves into NVIDIA Isaac Sim, a powerful platform for AI-driven robotics simulation and synthetic data generation. It covers Isaac ROS for VSLAM and perception, and Nav2 for advanced humanoid path planning.
- Isaac Sim overview
- Synthetic data generation
- Isaac ROS VSLAM and perception
- Nav2 path planning

### Module 4: Vision-Language-Action (VLA) & Capstone

Combines vision and language understanding with physical action. This module introduces systems like Whisper for speech input, language-to-action planning, and the design of VLA agents, culminating in a practical capstone project.
- Whisper for speech-to-text
- Language-to-action planning
- "Clean the room" end-to-end example
